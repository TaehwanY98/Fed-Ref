## Fed-Ref: Communication-Efficient Bayesian Fine Tuning with Reference Model

### Abstract
Federated learning(FL) is used for distributed scenarios to train artificial intelligence(AI) models while ensuring users' privacy. In federated learning scenario, the server generally never knows about users' data. This type of concept makes the AI training process efficient in terms of data privacy. However, about model performance, the federated AI model may not be sufficient to satisfy AI users' expectations. Moreover, there are many different needs to AI users. It is not easy to satisfy the whole users needs. This type of issues can be solved by AI model optimization and fine- tuning methods. we proposed reference model-based federated learning for optimal fine-tuning overcoming catastrophic forgetting on each round. This method is based on "bayesian parameter-efficient transfer learning" and enable to make model having previous round features and users' data features. As a result, this method achieve higher model performance than existing researches.

### Settings
| Environment set  | Settings for detail                             |
|------------------|-------------------------------------------------|
| FL framework     | Flower: a friendly federated learning framework |
| Language         | Python                                          |
| Operation System | Linux 24.04 LTS                                 |
| GPU              | Nvidia RTX 4090                                 |
| Tools            | Visual studio code                              |

### Results
